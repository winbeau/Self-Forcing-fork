{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 4 Reproduction: Temporal Attention Visualization\n",
    "\n",
    "This notebook reproduces Figure 4 from the DeepForcing paper, showing how attention weights\n",
    "are distributed across frames during autoregressive video generation.\n",
    "\n",
    "**Expected Pattern (from the paper):**\n",
    "- High attention at the beginning (initial frames provide strong context)\n",
    "- Attention drops in the middle\n",
    "- High attention again for recent/current frames\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Run the extraction script first:\n",
    "```bash\n",
    "python run_extraction.py \\\n",
    "    --config_path configs/self_forcing_dmd.yaml \\\n",
    "    --checkpoint_path checkpoints/self_forcing_dmd.pt \\\n",
    "    --output_path attention_cache.pt\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Attention Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cached attention weights\n",
    "CACHE_PATH = \"attention_cache.pt\"\n",
    "\n",
    "if not Path(CACHE_PATH).exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Attention cache not found at {CACHE_PATH}. \"\n",
    "        \"Please run run_extraction.py first.\"\n",
    "    )\n",
    "\n",
    "data = torch.load(CACHE_PATH, map_location='cpu')\n",
    "\n",
    "print(\"Loaded attention data:\")\n",
    "print(f\"  - Prompt: {data.get('prompt', 'N/A')}\")\n",
    "print(f\"  - Number of frames: {data.get('num_frames', 'N/A')}\")\n",
    "print(f\"  - Frame sequence length: {data.get('frame_seq_length', 'N/A')}\")\n",
    "print(f\"  - Frames per block: {data.get('num_frame_per_block', 'N/A')}\")\n",
    "print(f\"  - Number of transformer blocks: {data.get('num_transformer_blocks', 'N/A')}\")\n",
    "print(f\"  - Captured layer indices: {data.get('layer_indices', 'N/A')}\")\n",
    "print(f\"  - Number of captured weight tensors: {len(data['attention_weights'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the captured attention weights\n",
    "attention_weights = data['attention_weights']\n",
    "\n",
    "print(\"\\nCaptured attention weight tensors:\")\n",
    "for i, w in enumerate(attention_weights):\n",
    "    print(f\"  [{i}] Layer {w['layer_idx']}: \"\n",
    "          f\"attn_shape={w['attn_weights'].shape}, \"\n",
    "          f\"q_shape={w['q_shape']}, k_shape={w['k_shape']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Process Attention Weights\n",
    "\n",
    "The attention tensor shape is typically `(Batch, Heads, Query_Len, Key_Len)`.\n",
    "\n",
    "For Figure 4, we need to:\n",
    "1. Select a specific layer and head\n",
    "2. Average across the Query dimension to get the \"average attention per key position\"\n",
    "3. Map token positions to frame indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_attention_for_figure4(\n",
    "    attention_data: dict,\n",
    "    weight_idx: int = 0,\n",
    "    head_idx: int = 0,\n",
    "    frame_seq_length: int = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Process attention weights for Figure 4 visualization.\n",
    "    \n",
    "    Args:\n",
    "        attention_data: The loaded attention cache data\n",
    "        weight_idx: Index of the captured weight tensor to use\n",
    "        head_idx: Attention head index to visualize\n",
    "        frame_seq_length: Number of tokens per frame (default: from data)\n",
    "    \n",
    "    Returns:\n",
    "        frame_attention: Average attention weight per frame\n",
    "        frame_indices: Frame indices\n",
    "    \"\"\"\n",
    "    if frame_seq_length is None:\n",
    "        frame_seq_length = attention_data.get('frame_seq_length', 1560)\n",
    "    \n",
    "    # Get the attention weights: shape [B, N, Lq, Lk]\n",
    "    attn = attention_data['attention_weights'][weight_idx]['attn_weights']\n",
    "    print(f\"Raw attention shape: {attn.shape}\")\n",
    "    \n",
    "    # Extract batch=0, head=head_idx\n",
    "    # Shape: [Lq, Lk]\n",
    "    attn_matrix = attn[0, head_idx].float().numpy()\n",
    "    print(f\"Selected head attention shape: {attn_matrix.shape}\")\n",
    "    \n",
    "    lq, lk = attn_matrix.shape\n",
    "    \n",
    "    # Calculate number of frames\n",
    "    num_query_frames = lq // frame_seq_length\n",
    "    num_key_frames = lk // frame_seq_length\n",
    "    print(f\"Query frames: {num_query_frames}, Key frames: {num_key_frames}\")\n",
    "    \n",
    "    # Method 1: Average across all query positions\n",
    "    # This gives us \"how much attention does each key position receive on average\"\n",
    "    avg_attention_per_key = attn_matrix.mean(axis=0)  # [Lk]\n",
    "    \n",
    "    # Group by frame: average attention per frame\n",
    "    frame_attention = []\n",
    "    for f in range(num_key_frames):\n",
    "        start = f * frame_seq_length\n",
    "        end = min((f + 1) * frame_seq_length, lk)\n",
    "        frame_avg = avg_attention_per_key[start:end].mean()\n",
    "        frame_attention.append(frame_avg)\n",
    "    \n",
    "    frame_attention = np.array(frame_attention)\n",
    "    frame_indices = np.arange(num_key_frames)\n",
    "    \n",
    "    return frame_attention, frame_indices, attn_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the first captured attention tensor\n",
    "if len(attention_weights) > 0:\n",
    "    frame_attention, frame_indices, raw_attn = process_attention_for_figure4(\n",
    "        data, weight_idx=0, head_idx=0\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFrame attention shape: {frame_attention.shape}\")\n",
    "    print(f\"Frame indices: {frame_indices}\")\n",
    "    print(f\"Attention values: {frame_attention}\")\n",
    "else:\n",
    "    print(\"No attention weights captured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Plot Figure 4: Attention Distribution Across Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_figure4(\n",
    "    frame_attention: np.ndarray,\n",
    "    frame_indices: np.ndarray,\n",
    "    title: str = \"Temporal Attention Distribution (Figure 4)\",\n",
    "    save_path: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create the Figure 4 plot showing attention distribution across frames.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Plot the attention distribution\n",
    "    ax.plot(frame_indices, frame_attention, 'o-', linewidth=2, markersize=8, \n",
    "            color='#2E86AB', label='Average Attention')\n",
    "    \n",
    "    # Fill under the curve\n",
    "    ax.fill_between(frame_indices, frame_attention, alpha=0.3, color='#2E86AB')\n",
    "    \n",
    "    # Labels and title\n",
    "    ax.set_xlabel('Key Frame Index', fontsize=14)\n",
    "    ax.set_ylabel('Average Attention Weight', fontsize=14)\n",
    "    ax.set_title(title, fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Set x-axis to show all frame indices\n",
    "    ax.set_xticks(frame_indices)\n",
    "    ax.set_xlim(frame_indices[0] - 0.5, frame_indices[-1] + 0.5)\n",
    "    \n",
    "    # Add grid\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add annotations for interesting patterns\n",
    "    max_idx = np.argmax(frame_attention)\n",
    "    min_idx = np.argmin(frame_attention)\n",
    "    \n",
    "    ax.annotate(f'Max: {frame_attention[max_idx]:.4f}',\n",
    "                xy=(frame_indices[max_idx], frame_attention[max_idx]),\n",
    "                xytext=(10, 10), textcoords='offset points',\n",
    "                fontsize=10, color='green',\n",
    "                arrowprops=dict(arrowstyle='->', color='green', alpha=0.7))\n",
    "    \n",
    "    ax.annotate(f'Min: {frame_attention[min_idx]:.4f}',\n",
    "                xy=(frame_indices[min_idx], frame_attention[min_idx]),\n",
    "                xytext=(10, -20), textcoords='offset points',\n",
    "                fontsize=10, color='red',\n",
    "                arrowprops=dict(arrowstyle='->', color='red', alpha=0.7))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved figure to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Figure 4 plot\n",
    "if len(attention_weights) > 0:\n",
    "    fig = plot_figure4(\n",
    "        frame_attention, \n",
    "        frame_indices,\n",
    "        title=\"Temporal Self-Attention Distribution (Reproduction of Figure 4)\",\n",
    "        save_path=\"figure4_reproduction.png\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Multi-Head Analysis\n",
    "\n",
    "Let's visualize attention patterns across different heads to see if they show different behaviors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi_head_attention(\n",
    "    attention_data: dict,\n",
    "    weight_idx: int = 0,\n",
    "    num_heads_to_show: int = 8,\n",
    "    save_path: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot attention distributions for multiple heads.\n",
    "    \"\"\"\n",
    "    attn = attention_data['attention_weights'][weight_idx]['attn_weights']\n",
    "    total_heads = attn.shape[1]\n",
    "    num_heads_to_show = min(num_heads_to_show, total_heads)\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for head_idx in range(num_heads_to_show):\n",
    "        frame_attention, frame_indices, _ = process_attention_for_figure4(\n",
    "            attention_data, weight_idx=weight_idx, head_idx=head_idx\n",
    "        )\n",
    "        \n",
    "        ax = axes[head_idx]\n",
    "        ax.plot(frame_indices, frame_attention, 'o-', linewidth=1.5, markersize=4)\n",
    "        ax.fill_between(frame_indices, frame_attention, alpha=0.3)\n",
    "        ax.set_title(f'Head {head_idx}', fontsize=12)\n",
    "        ax.set_xlabel('Frame Index', fontsize=10)\n",
    "        ax.set_ylabel('Avg Attention', fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Attention Distribution Across Different Heads', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved figure to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot multi-head attention\n",
    "if len(attention_weights) > 0:\n",
    "    fig = plot_multi_head_attention(\n",
    "        data,\n",
    "        weight_idx=0,\n",
    "        num_heads_to_show=8,\n",
    "        save_path=\"figure4_multihead.png\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Full Attention Matrix Heatmap\n",
    "\n",
    "Visualize the complete attention matrix to understand Query-Key relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_heatmap(\n",
    "    attn_matrix: np.ndarray,\n",
    "    frame_seq_length: int = 1560,\n",
    "    downsample_factor: int = 100,\n",
    "    title: str = \"Attention Matrix Heatmap\",\n",
    "    save_path: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot the full attention matrix as a heatmap.\n",
    "    \"\"\"\n",
    "    # Downsample for visualization\n",
    "    lq, lk = attn_matrix.shape\n",
    "    \n",
    "    # Use frame-level averaging instead of token-level\n",
    "    num_q_frames = lq // frame_seq_length\n",
    "    num_k_frames = lk // frame_seq_length\n",
    "    \n",
    "    # Create frame-level attention matrix\n",
    "    frame_attn = np.zeros((num_q_frames, num_k_frames))\n",
    "    for qi in range(num_q_frames):\n",
    "        for ki in range(num_k_frames):\n",
    "            q_start, q_end = qi * frame_seq_length, min((qi + 1) * frame_seq_length, lq)\n",
    "            k_start, k_end = ki * frame_seq_length, min((ki + 1) * frame_seq_length, lk)\n",
    "            frame_attn[qi, ki] = attn_matrix[q_start:q_end, k_start:k_end].mean()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    im = ax.imshow(frame_attn, cmap='viridis', aspect='auto')\n",
    "    plt.colorbar(im, ax=ax, label='Attention Weight')\n",
    "    \n",
    "    ax.set_xlabel('Key Frame Index', fontsize=12)\n",
    "    ax.set_ylabel('Query Frame Index', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    # Set ticks\n",
    "    ax.set_xticks(range(num_k_frames))\n",
    "    ax.set_yticks(range(num_q_frames))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved figure to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot attention heatmap\n",
    "if len(attention_weights) > 0:\n",
    "    frame_seq_length = data.get('frame_seq_length', 1560)\n",
    "    fig = plot_attention_heatmap(\n",
    "        raw_attn,\n",
    "        frame_seq_length=frame_seq_length,\n",
    "        title=\"Frame-Level Attention Matrix (Head 0)\",\n",
    "        save_path=\"figure4_heatmap.png\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Head-Averaged Attention\n",
    "\n",
    "Average across all heads for a more robust visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_head_averaged_attention(\n",
    "    attention_data: dict,\n",
    "    weight_idx: int = 0,\n",
    "    save_path: str = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot attention distribution averaged across all heads.\n",
    "    \"\"\"\n",
    "    attn = attention_data['attention_weights'][weight_idx]['attn_weights']\n",
    "    frame_seq_length = attention_data.get('frame_seq_length', 1560)\n",
    "    total_heads = attn.shape[1]\n",
    "    \n",
    "    all_frame_attentions = []\n",
    "    \n",
    "    for head_idx in range(total_heads):\n",
    "        frame_attention, frame_indices, _ = process_attention_for_figure4(\n",
    "            attention_data, weight_idx=weight_idx, head_idx=head_idx\n",
    "        )\n",
    "        all_frame_attentions.append(frame_attention)\n",
    "    \n",
    "    all_frame_attentions = np.array(all_frame_attentions)  # [num_heads, num_frames]\n",
    "    \n",
    "    # Compute mean and std\n",
    "    mean_attention = all_frame_attentions.mean(axis=0)\n",
    "    std_attention = all_frame_attentions.std(axis=0)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Plot mean with error band\n",
    "    ax.plot(frame_indices, mean_attention, 'o-', linewidth=2, markersize=8,\n",
    "            color='#2E86AB', label='Mean across heads')\n",
    "    ax.fill_between(frame_indices,\n",
    "                    mean_attention - std_attention,\n",
    "                    mean_attention + std_attention,\n",
    "                    alpha=0.3, color='#2E86AB', label='Â±1 std')\n",
    "    \n",
    "    ax.set_xlabel('Key Frame Index', fontsize=14)\n",
    "    ax.set_ylabel('Average Attention Weight', fontsize=14)\n",
    "    ax.set_title('Head-Averaged Temporal Attention Distribution', fontsize=16, fontweight='bold')\n",
    "    ax.set_xticks(frame_indices)\n",
    "    ax.legend(fontsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        print(f\"Saved figure to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    return fig, mean_attention, std_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot head-averaged attention\n",
    "if len(attention_weights) > 0:\n",
    "    fig, mean_attn, std_attn = plot_head_averaged_attention(\n",
    "        data,\n",
    "        weight_idx=0,\n",
    "        save_path=\"figure4_head_averaged.png\"\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nMean attention per frame: {mean_attn}\")\n",
    "    print(f\"Std attention per frame: {std_attn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(attention_weights) > 0:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ATTENTION ANALYSIS SUMMARY\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nPrompt: {data.get('prompt', 'N/A')}\")\n",
    "    print(f\"Number of frames: {data.get('num_frames', 'N/A')}\")\n",
    "    print(f\"Captured layer: {attention_weights[0]['layer_idx']}\")\n",
    "    print(f\"\\nAttention distribution (head-averaged):\")\n",
    "    print(f\"  - Max attention frame: {np.argmax(mean_attn)} (value: {mean_attn.max():.6f})\")\n",
    "    print(f\"  - Min attention frame: {np.argmin(mean_attn)} (value: {mean_attn.min():.6f})\")\n",
    "    print(f\"  - First frame attention: {mean_attn[0]:.6f}\")\n",
    "    print(f\"  - Last frame attention: {mean_attn[-1]:.6f}\")\n",
    "    print(f\"\\nExpected pattern (from Figure 4):\")\n",
    "    print(\"  - High attention at beginning (context)\")\n",
    "    print(\"  - Lower attention in middle\")\n",
    "    print(\"  - High attention for recent frames\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Notes\n",
    "\n",
    "### Interpreting the Results\n",
    "\n",
    "1. **High attention at the start**: The model relies heavily on initial frames to establish scene context\n",
    "2. **Attention sink pattern**: Some models show high attention to the very first few tokens (attention sink)\n",
    "3. **Recent frame attention**: High attention to recent frames for temporal consistency\n",
    "4. **U-shaped pattern**: Classic pattern showing beginning + end attention, with a dip in the middle\n",
    "\n",
    "### Differences from Original Figure 4\n",
    "\n",
    "- The exact pattern depends on the checkpoint and inference settings\n",
    "- Different layers may show different patterns\n",
    "- KV cache rolling (for local attention) affects which frames are visible"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
