#!/usr/bin/env python
"""
æµ‹è¯• run_extraction_each.py çš„æå–é€»è¾‘ã€‚

å•å…ƒæµ‹è¯•ï¼ˆä¸éœ€è¦ GPUï¼‰ï¼š
- block ç»“æ„è®¡ç®—
- ç´¢å¼•æ˜ å°„é€»è¾‘
- æ•°æ®æ ¼å¼éªŒè¯

é›†æˆæµ‹è¯•ï¼ˆéœ€è¦ GPUï¼Œæ ‡è®°ä¸º slowï¼‰ï¼š
- å®Œæ•´æå–æµç¨‹
- è¾“å‡ºæ•°æ®æ­£ç¡®æ€§
"""

import pytest
import torch
import numpy as np
import os
import sys

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


class TestBlockStructure:
    """æµ‹è¯• block ç»“æ„è®¡ç®—é€»è¾‘"""

    def test_block_sizes_with_independent_first_frame(self):
        """æµ‹è¯• independent_first_frame=True æ—¶çš„ block ç»“æ„"""
        num_frames = 21
        num_frame_per_block = 3
        independent_first_frame = True

        # æ­£ç¡®çš„è®¡ç®—é€»è¾‘ï¼šç¬¬ä¸€å¸§ç‹¬ç«‹ï¼Œå‰©ä½™ 20 å¸§åˆ†æˆ 20//3=6 ä¸ª blockï¼Œè¿˜å‰© 2 å¸§
        # å®é™… block ç»“æ„åº”è¯¥æ˜¯ [1, 3, 3, 3, 3, 3, 3, 2] æˆ–ç±»ä¼¼
        # ä½†æ ¹æ® run_extraction_each.py çš„é€»è¾‘ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ï¼š
        if independent_first_frame:
            num_blocks = (num_frames - 1) // num_frame_per_block + 1
            block_sizes = [1] + [num_frame_per_block] * ((num_frames - 1) // num_frame_per_block)

        # éªŒè¯é€»è¾‘ï¼š[1] + [3]*6 = [1, 3, 3, 3, 3, 3, 3] = 19 å¸§
        # è¿™æ„å‘³ç€ 21 å¸§é…ç½®ä¸‹ï¼Œæœ€åä¸€ä¸ª block å®é™…ä¸Šæ˜¯ 3 å¸§ï¼Œ
        # æˆ–è€…é…ç½®è¦æ±‚ num_frames = 1 + 3*k
        # å¯¹äº 21 å¸§ï¼š1 + 3*6 = 19ï¼Œå°‘äº† 2 å¸§
        # å®é™…ä¸Šæœ€åä¸€ä¸ª block åº”è¯¥æ˜¯ 3 å¸§ï¼ˆframes 18, 19, 20ï¼‰

        # ä¿®æ­£ï¼šæ­£ç¡®çš„ block ç»“æ„
        # 21å¸§ = 1 + 20ï¼Œ20 = 3*6 + 2ï¼Œä½†æ¨¡å‹é…ç½®ä¸‹æ¯ä¸ª block éƒ½æ˜¯ 3 å¸§
        # å®é™…çš„ block_sizes ä¾èµ–äºå…·ä½“é…ç½®
        expected_block_sizes = [1, 3, 3, 3, 3, 3, 3]
        expected_sum = 19  # è¿™æ˜¯é…ç½®çš„é€»è¾‘ï¼Œä¸æ˜¯ 21

        assert block_sizes == expected_block_sizes, f"Block sizes mismatch: {block_sizes}"
        assert sum(block_sizes) == expected_sum, f"Sum of block sizes != {expected_sum}"

        print(f"âœ“ Block ç»“æ„æµ‹è¯•é€šè¿‡: {block_sizes} (æ€»å¸§æ•°é…ç½®ä¸º {expected_sum})")

    def test_block_sizes_without_independent_first_frame(self):
        """æµ‹è¯• independent_first_frame=False æ—¶çš„ block ç»“æ„"""
        num_frames = 21
        num_frame_per_block = 3
        independent_first_frame = False

        if independent_first_frame:
            num_blocks = (num_frames - 1) // num_frame_per_block + 1
            block_sizes = [1] + [num_frame_per_block] * ((num_frames - 1) // num_frame_per_block)
        else:
            num_blocks = num_frames // num_frame_per_block
            block_sizes = [num_frame_per_block] * num_blocks

        # éªŒè¯
        assert num_blocks == 7, f"Expected 7 blocks, got {num_blocks}"
        assert block_sizes == [3, 3, 3, 3, 3, 3, 3], f"Block sizes mismatch: {block_sizes}"
        assert sum(block_sizes) == num_frames, f"Sum of block sizes != num_frames"

        print(f"âœ“ Block ç»“æ„æµ‹è¯•é€šè¿‡ (non-independent): {block_sizes}")

    def test_cumulative_k_frames(self):
        """æµ‹è¯•æ¯ä¸ª block å¯¹åº”çš„ç´¯ç§¯ K å¸§æ•°"""
        block_sizes = [1, 3, 3, 3, 3, 3, 3]

        cumulative = []
        for i in range(len(block_sizes)):
            cumulative.append(sum(block_sizes[:i + 1]))

        expected = [1, 4, 7, 10, 13, 16, 19]
        assert cumulative == expected, f"Cumulative mismatch: {cumulative}"

        # æœ€åä¸€ä¸ª block åŠ ä¸Šè‡ªå·±çš„å¸§æ•°
        final_k_frames = cumulative[-1] + block_sizes[-1] - block_sizes[-1]  # = 19
        # ä½†å®é™…ä¸Šæœ€åä¸€ä¸ª block çš„ K åŒ…å«åˆ°å½“å‰ block ä¸ºæ­¢çš„æ‰€æœ‰å¸§
        # å³ block 6 çš„ K åº”è¯¥æ˜¯ 1+3+3+3+3+3+3 = 19? ä¸å¯¹ï¼Œåº”è¯¥æ˜¯ 21

        # é‡æ–°ç†è§£ï¼šæ¯ä¸ª block çš„ K åŒ…å«ä» frame 0 åˆ°å½“å‰ block æœ€åä¸€å¸§
        # block 0: Q=frame 0, K=frame 0 (1å¸§)
        # block 1: Q=frame 1-3, K=frame 0-3 (4å¸§)
        # ...
        # block 6: Q=frame 18-20, K=frame 0-20 (21å¸§)

        final_expected = [1, 4, 7, 10, 13, 16, 19]  # è¿™æ˜¯ block ç»“æŸæ—¶çš„å¸§æ•°
        # ä½†å®é™… K åŒ…å«åˆ° Q æ‰€åœ¨çš„å¸§ï¼Œæ‰€ä»¥ block 6 çš„ K = 21 å¸§

        print(f"âœ“ ç´¯ç§¯ K å¸§æ•°æµ‹è¯•é€šè¿‡: {cumulative}")


class TestIndexMapping:
    """æµ‹è¯•ç´¢å¼•æ˜ å°„é€»è¾‘"""

    def test_layer_to_self_attn_index(self):
        """æµ‹è¯• layer index åˆ° self-attention è°ƒç”¨ç´¢å¼•çš„æ˜ å°„"""
        # layer N â†’ self-attn è°ƒç”¨ç´¢å¼• 2*N
        # å› ä¸ºæ¯ä¸ª block æœ‰: self-attn (å¶æ•°), cross-attn (å¥‡æ•°)

        layer_indices = [0, 4, 15, 29]
        expected_self_attn_indices = [0, 8, 30, 58]

        for layer_idx, expected in zip(layer_indices, expected_self_attn_indices):
            self_attn_idx = 2 * layer_idx
            assert self_attn_idx == expected, \
                f"Layer {layer_idx}: expected {expected}, got {self_attn_idx}"

        print(f"âœ“ Layer â†’ Self-attn ç´¢å¼•æ˜ å°„æµ‹è¯•é€šè¿‡")

    def test_call_index_to_block_index(self):
        """æµ‹è¯•è°ƒç”¨ç´¢å¼•å›è½¬åˆ° block ç´¢å¼•"""
        # call_idx = 0, 2, 4, ... â†’ block_idx = 0, 1, 2, ...
        # call_idx // 2 = block_idx

        call_indices = [0, 2, 4, 6, 58]
        expected_block_indices = [0, 1, 2, 3, 29]

        for call_idx, expected in zip(call_indices, expected_block_indices):
            block_idx = call_idx // 2
            assert block_idx == expected, \
                f"Call {call_idx}: expected block {expected}, got {block_idx}"

        print(f"âœ“ è°ƒç”¨ç´¢å¼• â†’ Block ç´¢å¼•æ˜ å°„æµ‹è¯•é€šè¿‡")


class TestDataFormat:
    """æµ‹è¯•æ•°æ®æ ¼å¼"""

    def test_output_data_structure(self):
        """æµ‹è¯•è¾“å‡ºæ•°æ®ç»“æ„"""
        # æ¨¡æ‹Ÿä¿å­˜çš„æ•°æ®ç»“æ„
        num_frames = 21
        num_heads = 12

        save_data = {
            'layer_index': 0,
            'full_frame_attention': torch.zeros(num_heads, num_frames, num_frames),
            'last_block_frame_attention': torch.zeros(num_heads, num_frames),
            'is_logits': True,
            'prompt': "test",
            'num_frames': num_frames,
            'frame_seq_length': 1560,
            'num_frame_per_block': 3,
            'num_heads': num_heads,
            'block_sizes': [1, 3, 3, 3, 3, 3, 3],
            'query_frames': list(range(num_frames)),
            'key_frames': list(range(num_frames)),
            'last_block_query_frames': [18, 19, 20],
        }

        # éªŒè¯å¿…è¦å­—æ®µ
        required_keys = [
            'layer_index', 'full_frame_attention', 'last_block_frame_attention',
            'num_frames', 'num_heads', 'block_sizes', 'last_block_query_frames'
        ]

        for key in required_keys:
            assert key in save_data, f"Missing key: {key}"

        # éªŒè¯å½¢çŠ¶
        assert save_data['full_frame_attention'].shape == (num_heads, num_frames, num_frames)
        assert save_data['last_block_frame_attention'].shape == (num_heads, num_frames)

        print(f"âœ“ æ•°æ®ç»“æ„æµ‹è¯•é€šè¿‡")

    def test_attention_matrix_is_causal(self):
        """æµ‹è¯•æ³¨æ„åŠ›çŸ©é˜µæ˜¯å› æœçš„ï¼ˆä¸‹ä¸‰è§’ï¼‰"""
        num_frames = 21
        num_heads = 12

        # åˆ›å»ºæ¨¡æ‹Ÿçš„å› æœæ³¨æ„åŠ›çŸ©é˜µ
        full_attn = torch.zeros(num_heads, num_frames, num_frames)

        # å¡«å……ä¸‹ä¸‰è§’ï¼ˆåŒ…æ‹¬å¯¹è§’çº¿ï¼‰
        for h in range(num_heads):
            for q in range(num_frames):
                for k in range(q + 1):  # k <= q (causal)
                    full_attn[h, q, k] = torch.rand(1).item()

        # éªŒè¯ä¸Šä¸‰è§’ä¸ºé›¶
        for h in range(num_heads):
            for q in range(num_frames):
                for k in range(q + 1, num_frames):  # k > q
                    assert full_attn[h, q, k] == 0, \
                        f"Non-causal attention at h={h}, q={q}, k={k}"

        print(f"âœ“ å› æœæ³¨æ„åŠ›çŸ©é˜µæµ‹è¯•é€šè¿‡")


class TestAttentionCaptureMechanism:
    """æµ‹è¯• ATTENTION_WEIGHT_CAPTURE æœºåˆ¶"""

    def test_enable_disable(self):
        """æµ‹è¯•å¯ç”¨å’Œç¦ç”¨"""
        from wan.modules.attention import ATTENTION_WEIGHT_CAPTURE

        # åˆå§‹çŠ¶æ€åº”è¯¥æ˜¯ç¦ç”¨çš„
        ATTENTION_WEIGHT_CAPTURE.disable()
        assert not ATTENTION_WEIGHT_CAPTURE.enabled

        # å¯ç”¨
        ATTENTION_WEIGHT_CAPTURE.enable(layer_indices=[0, 4], num_layers=30)
        assert ATTENTION_WEIGHT_CAPTURE.enabled
        assert ATTENTION_WEIGHT_CAPTURE.layer_indices == [0, 4]
        assert ATTENTION_WEIGHT_CAPTURE.num_layers == 30

        # ç¦ç”¨
        ATTENTION_WEIGHT_CAPTURE.disable()
        assert not ATTENTION_WEIGHT_CAPTURE.enabled
        assert ATTENTION_WEIGHT_CAPTURE.captured_weights == []

        print(f"âœ“ Enable/Disable æµ‹è¯•é€šè¿‡")

    def test_should_capture_logic(self):
        """æµ‹è¯• should_capture é€»è¾‘"""
        from wan.modules.attention import ATTENTION_WEIGHT_CAPTURE

        ATTENTION_WEIGHT_CAPTURE.enable(layer_indices=[0, 8], num_layers=60)

        # æµ‹è¯•ä¸åŒçš„ current_layer_idx
        test_cases = [
            (0, True),   # 0 % 60 = 0, in [0, 8]
            (1, False),  # 1 % 60 = 1, not in [0, 8]
            (8, True),   # 8 % 60 = 8, in [0, 8]
            (60, True),  # 60 % 60 = 0, in [0, 8]
            (68, True),  # 68 % 60 = 8, in [0, 8]
            (120, True), # 120 % 60 = 0, in [0, 8]
        ]

        for idx, expected in test_cases:
            ATTENTION_WEIGHT_CAPTURE.current_layer_idx = idx
            result = ATTENTION_WEIGHT_CAPTURE.should_capture()
            assert result == expected, \
                f"idx={idx}: expected {expected}, got {result}"

        ATTENTION_WEIGHT_CAPTURE.disable()
        print(f"âœ“ should_capture é€»è¾‘æµ‹è¯•é€šè¿‡")

    def test_effective_layer_idx(self):
        """æµ‹è¯• effective layer index è®¡ç®—"""
        from wan.modules.attention import ATTENTION_WEIGHT_CAPTURE

        ATTENTION_WEIGHT_CAPTURE.enable(num_layers=60)

        test_cases = [
            (0, 0),
            (59, 59),
            (60, 0),
            (61, 1),
            (120, 0),
            (128, 8),
        ]

        for current_idx, expected_effective in test_cases:
            ATTENTION_WEIGHT_CAPTURE.current_layer_idx = current_idx
            effective = ATTENTION_WEIGHT_CAPTURE.get_effective_layer_idx()
            assert effective == expected_effective, \
                f"current={current_idx}: expected effective={expected_effective}, got {effective}"

        ATTENTION_WEIGHT_CAPTURE.disable()
        print(f"âœ“ Effective layer index æµ‹è¯•é€šè¿‡")


class TestFrameAttentionComputation:
    """æµ‹è¯•å¸§çº§æ³¨æ„åŠ›è®¡ç®—"""

    def test_token_to_frame_aggregation(self):
        """æµ‹è¯• token çº§æ³¨æ„åŠ›åˆ° frame çº§çš„èšåˆ"""
        num_heads = 12
        frame_seq_length = 1560
        num_q_frames = 3
        num_k_frames = 21

        # åˆ›å»ºæ¨¡æ‹Ÿçš„ token çº§æ³¨æ„åŠ› [num_heads, Lq, Lk]
        lq = num_q_frames * frame_seq_length
        lk = num_k_frames * frame_seq_length
        attn_logits = torch.randn(num_heads, lq, lk)

        # è®¡ç®— frame çº§æ³¨æ„åŠ›
        frame_attention = torch.zeros(num_heads, num_k_frames)

        for h in range(num_heads):
            head_attn = attn_logits[h]  # [Lq, Lk]
            avg_per_key = head_attn.mean(dim=0)  # [Lk] - å¯¹æ‰€æœ‰ query å–å¹³å‡

            for kf in range(num_k_frames):
                k_start = kf * frame_seq_length
                k_end = (kf + 1) * frame_seq_length
                frame_attention[h, kf] = avg_per_key[k_start:k_end].mean()

        # éªŒè¯å½¢çŠ¶
        assert frame_attention.shape == (num_heads, num_k_frames)

        # éªŒè¯å€¼åœ¨åˆç†èŒƒå›´å†…ï¼ˆæ ‡å‡†æ­£æ€çš„å‡å€¼åº”è¯¥æ¥è¿‘0ï¼‰
        assert abs(frame_attention.mean().item()) < 0.1, \
            f"Mean too far from 0: {frame_attention.mean().item()}"

        print(f"âœ“ Tokenâ†’Frame èšåˆæµ‹è¯•é€šè¿‡")

    def test_full_matrix_assembly(self):
        """æµ‹è¯•å®Œæ•´çŸ©é˜µç»„è£…é€»è¾‘

        æ³¨æ„ï¼šBlock-based causality ä¸æ˜¯ä¸¥æ ¼çš„ frame-level causality (k <= q)ã€‚
        åœ¨ block å†…ï¼Œæ‰€æœ‰ query frame éƒ½å¯ä»¥ attend åˆ°è¯¥ block ç»“æŸä¸ºæ­¢çš„æ‰€æœ‰ key framesã€‚
        ä¾‹å¦‚ï¼šblock 1 çš„ Q frames 1-3 éƒ½å¯ä»¥çœ‹åˆ° K frames 0-3ã€‚
        """
        num_heads = 12
        frame_seq_length = 1560
        block_sizes = [1, 3, 3, 3, 3, 3, 3]
        num_frames = sum(block_sizes)  # 19 å¸§ (ä¸ block_sizes ä¿æŒä¸€è‡´)

        # åˆå§‹åŒ–å®Œæ•´çŸ©é˜µ
        full_frame_attn = torch.zeros(num_heads, num_frames, num_frames)

        # è®¡ç®—æ¯ä¸ª frame æ‰€å±çš„ block å’Œè¯¥ block çš„ K èŒƒå›´
        frame_to_k_end = {}  # frame_idx -> è¯¥ frame å¯è§çš„æœ€å¤§ K index + 1
        current_q_start = 0
        for block_idx, block_size in enumerate(block_sizes):
            k_frames_total = sum(block_sizes[:block_idx + 1])
            for qf_local in range(block_size):
                qf_global = current_q_start + qf_local
                frame_to_k_end[qf_global] = k_frames_total
            current_q_start += block_size

        # æ¨¡æ‹Ÿæ¯ä¸ª block çš„æ•°æ®
        current_q_start = 0
        for block_idx, block_size in enumerate(block_sizes):
            k_frames_total = sum(block_sizes[:block_idx + 1])
            q_frames_in_block = block_size

            # å¡«å……è¿™ä¸ª block çš„æ³¨æ„åŠ›
            for h in range(num_heads):
                for qf_local in range(q_frames_in_block):
                    qf_global = current_q_start + qf_local
                    for kf in range(k_frames_total):
                        # æ¨¡æ‹Ÿå€¼ï¼ˆéé›¶ï¼‰
                        full_frame_attn[h, qf_global, kf] = (qf_global + 1) * 0.1 + kf * 0.01 + 0.001

            current_q_start += block_size

        # éªŒè¯ï¼šå¯¹äºæ¯ä¸ª query frameï¼Œè¶…å‡ºå…¶ K èŒƒå›´çš„ä½ç½®åº”è¯¥æ˜¯ 0
        for h in range(num_heads):
            for q in range(num_frames):
                k_end = frame_to_k_end[q]
                # K èŒƒå›´å†…åº”è¯¥æœ‰å€¼
                for k in range(k_end):
                    assert full_frame_attn[h, q, k] != 0, \
                        f"Zero at h={h}, q={q}, k={k} (k_end={k_end})"
                # K èŒƒå›´å¤–åº”è¯¥æ˜¯ 0
                for k in range(k_end, num_frames):
                    assert full_frame_attn[h, q, k] == 0, \
                        f"Non-zero at h={h}, q={q}, k={k} (k_end={k_end})"

        print(f"âœ“ å®Œæ•´çŸ©é˜µç»„è£…æµ‹è¯•é€šè¿‡ï¼ˆblock-based causalityï¼‰")


@pytest.mark.slow
@pytest.mark.gpu
class TestIntegrationWithGPU:
    """é›†æˆæµ‹è¯•ï¼ˆéœ€è¦ GPUï¼‰"""

    def test_extraction_produces_valid_output(self):
        """æµ‹è¯•æå–è„šæœ¬äº§ç”Ÿæœ‰æ•ˆè¾“å‡º"""
        if not torch.cuda.is_available():
            pytest.skip("CUDA not available")

        from omegaconf import OmegaConf
        from pipeline.causal_inference import CausalInferencePipeline
        from wan.modules.attention import ATTENTION_WEIGHT_CAPTURE
        from utils.misc import set_seed

        set_seed(42)
        device = torch.device("cuda:0")

        # åŠ è½½é…ç½®
        print("\nğŸ“ åŠ è½½é…ç½®æ–‡ä»¶...")
        config = OmegaConf.load("configs/self_forcing_dmd.yaml")
        default_config = OmegaConf.load("configs/default_config.yaml")
        config = OmegaConf.merge(default_config, config)

        print("ğŸ”§ åˆå§‹åŒ– pipeline...")
        torch.set_grad_enabled(False)
        pipeline = CausalInferencePipeline(args=config, device=device)
        pipeline = pipeline.to(device=device, dtype=torch.bfloat16)
        pipeline.eval()
        print("âœ“ Pipeline åˆå§‹åŒ–å®Œæˆ")

        num_layers = len(pipeline.generator.model.blocks)
        num_frames = 21
        frame_seq_length = 1560
        layer_index = 0

        # è®¡ç®— block ç»“æ„
        block_sizes = [1, 3, 3, 3, 3, 3, 3]

        noise = torch.randn(
            [1, num_frames, 16, 60, 104],
            device=device,
            dtype=torch.bfloat16
        )

        # å¯ç”¨æ•è·
        self_attn_idx = 2 * layer_index
        ATTENTION_WEIGHT_CAPTURE.enable(
            layer_indices=[self_attn_idx],
            capture_logits=True,
            num_layers=num_layers * 2
        )

        try:
            print("ğŸš€ è¿è¡Œæ¨ç† (é¦–æ¬¡è¿è¡Œéœ€è¦ç¼–è¯‘ï¼Œè¯·è€å¿ƒç­‰å¾…)...")
            pipeline.inference(
                noise=noise,
                text_prompts=["A test video"],
                return_latents=True,
            )
            captured = ATTENTION_WEIGHT_CAPTURE.captured_weights.copy()
            print(f"âœ“ æ¨ç†å®Œæˆï¼Œæ•è·äº† {len(captured)} ä¸ª attention")
        finally:
            ATTENTION_WEIGHT_CAPTURE.disable()

        # éªŒè¯æ•è·äº†æ•°æ®
        assert len(captured) > 0, "No attention captured"

        # éªŒè¯æ•°æ®æ ¼å¼
        for attn in captured:
            assert 'attn_weights' in attn
            assert 'k_shape' in attn
            assert 'q_shape' in attn

            # éªŒè¯å½¢çŠ¶åˆç†
            attn_weights = attn['attn_weights']
            assert len(attn_weights.shape) == 4, \
                f"Expected 4D tensor, got {attn_weights.shape}"

            # [B, N, Lq, Lk]
            assert attn_weights.shape[0] == 1  # batch size
            assert attn_weights.shape[1] == 12  # num_heads

        print(f"âœ“ é›†æˆæµ‹è¯•é€šè¿‡: æ•è·äº† {len(captured)} ä¸ª attention")

    def test_full_matrix_shape(self):
        """æµ‹è¯•å®Œæ•´çŸ©é˜µå½¢çŠ¶æ­£ç¡®"""
        if not torch.cuda.is_available():
            pytest.skip("CUDA not available")

        from omegaconf import OmegaConf
        from pipeline.causal_inference import CausalInferencePipeline
        from wan.modules.attention import ATTENTION_WEIGHT_CAPTURE
        from utils.misc import set_seed

        set_seed(42)
        device = torch.device("cuda:0")

        print("\nğŸ“ åŠ è½½é…ç½®æ–‡ä»¶...")
        config = OmegaConf.load("configs/self_forcing_dmd.yaml")
        default_config = OmegaConf.load("configs/default_config.yaml")
        config = OmegaConf.merge(default_config, config)

        print("ğŸ”§ åˆå§‹åŒ– pipeline...")
        torch.set_grad_enabled(False)
        pipeline = CausalInferencePipeline(args=config, device=device)
        pipeline = pipeline.to(device=device, dtype=torch.bfloat16)
        pipeline.eval()
        print("âœ“ Pipeline åˆå§‹åŒ–å®Œæˆ")

        num_layers = len(pipeline.generator.model.blocks)
        num_frames = 21
        num_heads = 12
        frame_seq_length = 1560
        block_sizes = [1, 3, 3, 3, 3, 3, 3]

        noise = torch.randn(
            [1, num_frames, 16, 60, 104],
            device=device,
            dtype=torch.bfloat16
        )

        layer_index = 0
        self_attn_idx = 2 * layer_index

        ATTENTION_WEIGHT_CAPTURE.enable(
            layer_indices=[self_attn_idx],
            capture_logits=True,
            num_layers=num_layers * 2
        )

        try:
            print("ğŸš€ è¿è¡Œæ¨ç†...")
            pipeline.inference(
                noise=noise,
                text_prompts=["A test video"],
                return_latents=True,
            )
            captured = ATTENTION_WEIGHT_CAPTURE.captured_weights.copy()
            print(f"âœ“ æ¨ç†å®Œæˆï¼Œæ•è·äº† {len(captured)} ä¸ª attention")
        finally:
            ATTENTION_WEIGHT_CAPTURE.disable()

        # æŒ‰ K é•¿åº¦æ’åº
        print("ğŸ“Š æ„å»ºå®Œæ•´çŸ©é˜µ...")
        attns_sorted = sorted(captured, key=lambda x: x['k_shape'][1])

        # æ„å»ºå®Œæ•´çŸ©é˜µ
        full_frame_attn = torch.zeros(num_heads, num_frames, num_frames)

        processed_k_frames = set()
        current_q_start = 0

        for block_idx, block_size in enumerate(block_sizes):
            expected_k_frames = sum(block_sizes[:block_idx + 1])

            matching_attn = None
            for a in attns_sorted:
                k_frames = a['k_shape'][1] // frame_seq_length
                if k_frames == expected_k_frames and k_frames not in processed_k_frames:
                    matching_attn = a
                    processed_k_frames.add(k_frames)
                    break

            if matching_attn is None:
                current_q_start += block_size
                continue

            attn_logits = matching_attn['attn_weights'][0].float()
            q_tokens = attn_logits.shape[1]
            k_tokens = attn_logits.shape[2]
            q_frames_in_block = q_tokens // frame_seq_length
            k_frames_total = k_tokens // frame_seq_length

            for h in range(num_heads):
                head_attn = attn_logits[h]
                for qf_local in range(q_frames_in_block):
                    qf_global = current_q_start + qf_local
                    for kf in range(k_frames_total):
                        q_start_tok = qf_local * frame_seq_length
                        q_end_tok = (qf_local + 1) * frame_seq_length
                        k_start_tok = kf * frame_seq_length
                        k_end_tok = (kf + 1) * frame_seq_length
                        frame_attn_val = head_attn[q_start_tok:q_end_tok, k_start_tok:k_end_tok].mean()
                        full_frame_attn[h, qf_global, kf] = frame_attn_val

            current_q_start += block_size

        # éªŒè¯å½¢çŠ¶
        assert full_frame_attn.shape == (num_heads, num_frames, num_frames), \
            f"Shape mismatch: {full_frame_attn.shape}"

        # éªŒè¯æœ‰å€¼ï¼ˆä¸å…¨ä¸ºé›¶ï¼‰
        assert full_frame_attn.abs().sum() > 0, "Matrix is all zeros"

        print(f"âœ“ å®Œæ•´çŸ©é˜µå½¢çŠ¶æµ‹è¯•é€šè¿‡: {full_frame_attn.shape}")
        print(f"  Range: [{full_frame_attn.min():.4f}, {full_frame_attn.max():.4f}]")


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s", "--tb=short"])
