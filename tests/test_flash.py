import torch
import flash_attn
from flash_attn import flash_attn_func

print(f"âœ… Flash Attention ç‰ˆæœ¬: {flash_attn.__version__}")
print(f"âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
print(f"âœ… CUDA è®¾å¤‡: {torch.cuda.get_device_name(0)}")

# 1. å‡†å¤‡æµ‹è¯•æ•°æ® (å¿…é¡»æ˜¯ CUDA + float16 æˆ– bfloat16)
# Shape: (Batch_Size, Seq_Len, Num_Heads, Head_Dim)
batch_size = 2
seq_len = 1024
n_heads = 8
head_dim = 64

dtype = torch.float16 # FlashAttn å¿…é¡»è·‘åœ¨ fp16 æˆ– bf16 ä¸‹
device = "cuda"

print("\nğŸš€ æ­£åœ¨ç”Ÿæˆéšæœº Tensor...")
q = torch.randn((batch_size, seq_len, n_heads, head_dim), device=device, dtype=dtype)
k = torch.randn((batch_size, seq_len, n_heads, head_dim), device=device, dtype=dtype)
v = torch.randn((batch_size, seq_len, n_heads, head_dim), device=device, dtype=dtype)

# 2. è°ƒç”¨ Flash Attention
print("âš¡ æ­£åœ¨æ‰§è¡Œ Flash Attention è®¡ç®—...")
try:
    # è°ƒç”¨æ ¸å¿ƒå‡½æ•°
    output = flash_attn_func(q, k, v)
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    expected_shape = (batch_size, seq_len, n_heads, head_dim)
    assert output.shape == expected_shape, f"å½¢çŠ¶é”™è¯¯: {output.shape} != {expected_shape}"
    
    print(f"ğŸ‰ æˆåŠŸï¼è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print("âœ¨ Flash Attention å®‰è£…å®Œç¾ï¼Œå¯ä»¥æŠ•å…¥æˆ˜æ–—äº†ï¼")

except Exception as e:
    print(f"\nâŒ å‡ºé”™äº†: {e}")
    print("è¿™é€šå¸¸æ˜¯å› ä¸ºæ˜¾å¡æ¶æ„å¤ªæ—§ï¼ˆéœ€Ampereä»¥ä¸Šï¼‰æˆ– PyTorch/CUDA ç‰ˆæœ¬ä¸åŒ¹é…ã€‚")
