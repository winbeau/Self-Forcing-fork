"""Flash Attention åŠŸèƒ½æµ‹è¯•"""

import pytest
import torch


@pytest.mark.slow
@pytest.mark.gpu
def test_flash_attention_basic():
    """æµ‹è¯• Flash Attention åŸºæœ¬åŠŸèƒ½"""
    if not torch.cuda.is_available():
        pytest.skip("CUDA not available")

    try:
        import flash_attn
        from flash_attn import flash_attn_func
    except ImportError:
        pytest.skip("flash-attn not installed")

    print(f"\nâœ… Flash Attention ç‰ˆæœ¬: {flash_attn.__version__}")
    print(f"âœ… PyTorch ç‰ˆæœ¬: {torch.__version__}")
    print(f"âœ… CUDA è®¾å¤‡: {torch.cuda.get_device_name(0)}")

    # å‡†å¤‡æµ‹è¯•æ•°æ® (å¿…é¡»æ˜¯ CUDA + float16 æˆ– bfloat16)
    batch_size = 2
    seq_len = 1024
    n_heads = 8
    head_dim = 64
    dtype = torch.float16
    device = "cuda"

    print("\nğŸš€ ç”Ÿæˆéšæœº Tensor...")
    q = torch.randn((batch_size, seq_len, n_heads, head_dim), device=device, dtype=dtype)
    k = torch.randn((batch_size, seq_len, n_heads, head_dim), device=device, dtype=dtype)
    v = torch.randn((batch_size, seq_len, n_heads, head_dim), device=device, dtype=dtype)

    print("âš¡ æ‰§è¡Œ Flash Attention è®¡ç®—...")
    output = flash_attn_func(q, k, v)

    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    expected_shape = (batch_size, seq_len, n_heads, head_dim)
    assert output.shape == expected_shape, f"å½¢çŠ¶é”™è¯¯: {output.shape} != {expected_shape}"

    print(f"âœ“ Flash Attention æµ‹è¯•é€šè¿‡ï¼Œè¾“å‡ºå½¢çŠ¶: {output.shape}")
