#!/usr/bin/env python
"""
æµ‹è¯• ATTENTION_WEIGHT_CAPTURE æœºåˆ¶æ˜¯å¦æ­£å¸¸å·¥ä½œã€‚

éªŒè¯ï¼š
1. æ¨¡å—åŒ–ç´¢å¼•æ­£ç¡®å·¥ä½œ
2. èƒ½æ•èŽ·åˆ°å¤šä¸ª block/timestep çš„ attention
3. æœ€åŽä¸€ä¸ª block çš„ K åŒ…å«æ‰€æœ‰åŽ†å²å¸§

æ³¨æ„ï¼šGPU æµ‹è¯•éœ€è¦ --run-slow é€‰é¡¹
"""

import pytest
import torch
import sys
import os

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


class TestAttentionCaptureUnit:
    """å•å…ƒæµ‹è¯•ï¼ˆä¸éœ€è¦ GPUï¼‰"""

    def test_modular_index(self):
        """æµ‹è¯•æ¨¡å—åŒ–ç´¢å¼•æ­£ç¡®å·¥ä½œ"""
        from wan.modules.attention import AttentionWeightCapture

        capture = AttentionWeightCapture()
        capture.enable(layer_indices=[0, 4], num_layers=30)

        # æ¨¡æ‹Ÿå¤šä¸ª step çš„è°ƒç”¨
        results = []
        for step in range(3):  # 3ä¸ª step
            for layer in range(30):  # 30å±‚
                capture.current_layer_idx = step * 30 + layer
                if capture.should_capture():
                    results.append((step, layer, capture.get_effective_layer_idx()))

        # åº”è¯¥æ¯ä¸ª step éƒ½æ•èŽ· layer 0 å’Œ 4
        assert len(results) == 6, f"Expected 6 captures, got {len(results)}"

        # éªŒè¯æ•èŽ·çš„æ˜¯æ­£ç¡®çš„å±‚
        for step, layer, effective in results:
            assert effective in [0, 4], f"Unexpected effective layer: {effective}"
            assert layer == effective, f"Layer mismatch: {layer} vs {effective}"

        print(f"âœ“ æ¨¡å—åŒ–ç´¢å¼•æµ‹è¯•é€šè¿‡: æ•èŽ·äº† {len(results)} ä¸ª attention")
        print(f"  æ•èŽ·è¯¦æƒ…: {results}")


@pytest.mark.slow
@pytest.mark.gpu
class TestAttentionCaptureGPU:
    """GPU é›†æˆæµ‹è¯•ï¼ˆéœ€è¦ --run-slowï¼‰"""

    def test_capture_shape_grows_with_kv_cache(self):
        """æµ‹è¯• KV cache æ¨¡å¼ä¸‹ï¼ŒK çš„é•¿åº¦éšç€ block å¢žåŠ è€Œå¢žé•¿"""
        if not torch.cuda.is_available():
            pytest.skip("CUDA not available")

        from omegaconf import OmegaConf
        from pipeline.causal_inference import CausalInferencePipeline
        from wan.modules.attention import ATTENTION_WEIGHT_CAPTURE
        from utils.misc import set_seed

        print("\nðŸ“ åŠ è½½é…ç½®...")
        set_seed(42)
        device = torch.device("cuda:0")

        # åŠ è½½é…ç½®
        config = OmegaConf.load("configs/self_forcing_dmd.yaml")
        default_config = OmegaConf.load("configs/default_config.yaml")
        config = OmegaConf.merge(default_config, config)

        torch.set_grad_enabled(False)
        pipeline = CausalInferencePipeline(args=config, device=device)
        pipeline = pipeline.to(device=device, dtype=torch.bfloat16)
        pipeline.eval()

        num_layers = len(pipeline.generator.model.blocks)
        num_frames = 21
        num_frame_per_block = config.get('num_frame_per_block', 3)

        noise = torch.randn(
            [1, num_frames, 16, 60, 104],
            device=device,
            dtype=torch.bfloat16
        )

        # åªæ•èŽ· layer 0
        ATTENTION_WEIGHT_CAPTURE.enable(
            layer_indices=[0],
            capture_logits=True,
            num_layers=num_layers
        )

        try:
            pipeline.inference(
                noise=noise,
                text_prompts=["A test video"],
                return_latents=True,
            )
            captured = ATTENTION_WEIGHT_CAPTURE.captured_weights.copy()
        finally:
            ATTENTION_WEIGHT_CAPTURE.disable()

        print(f"\næ•èŽ·åˆ° {len(captured)} ä¸ª attention çŸ©é˜µ")

        # éªŒè¯æ•èŽ·äº†å¤šä¸ª attention
        # æ³¨æ„ï¼šå®žé™…æ•èŽ·æ•°é‡å¯èƒ½å› ä¸º cross attention ç­‰åŽŸå› è€Œæ›´å¤š
        assert len(captured) > 0, "Should capture some attention"

        # åˆ†æž K çš„é•¿åº¦å˜åŒ–
        k_lengths = [c['k_shape'][1] for c in captured]
        print(f"K é•¿åº¦åºåˆ—ï¼ˆå‰12ä¸ªï¼‰: {k_lengths[:12]}")

        # K é•¿åº¦åº”è¯¥éšç€ block å¢žåŠ è€Œå¢žé•¿
        frame_seq_length = 60 * 104 // 4  # 1560

        # æ£€æŸ¥ K é•¿åº¦çš„æœ€å¤§å€¼ï¼ˆæœ€åŽä¸€ä¸ª blockï¼‰
        max_k_len = max(k_lengths)
        expected_k_len = num_frames * frame_seq_length  # æ‰€æœ‰å¸§

        print(f"æœ€å¤§ K é•¿åº¦: {max_k_len}")
        print(f"æœŸæœ›çš„ K é•¿åº¦ï¼ˆåŒ…å«æ‰€æœ‰å¸§ï¼‰: {expected_k_len}")

        # éªŒè¯æœ€åŽä¸€ä¸ª block çš„ K åŒ…å«æ‰€æœ‰åŽ†å²å¸§
        min_k_len = (num_frames - num_frame_per_block) * frame_seq_length
        assert max_k_len >= min_k_len, \
            f"Max K length {max_k_len} is less than expected minimum {min_k_len}"

        print(f"âœ“ KV cache æµ‹è¯•é€šè¿‡: K é•¿åº¦æœ€å¤§ä¸º {max_k_len // frame_seq_length} å¸§")

    def test_last_block_has_full_history(self):
        """æµ‹è¯•æœ€åŽä¸€ä¸ª block çš„ attention åŒ…å«å®Œæ•´åŽ†å²"""
        if not torch.cuda.is_available():
            pytest.skip("CUDA not available")

        from omegaconf import OmegaConf
        from pipeline.causal_inference import CausalInferencePipeline
        from wan.modules.attention import ATTENTION_WEIGHT_CAPTURE
        from utils.misc import set_seed

        set_seed(42)
        device = torch.device("cuda:0")

        config = OmegaConf.load("configs/self_forcing_dmd.yaml")
        default_config = OmegaConf.load("configs/default_config.yaml")
        config = OmegaConf.merge(default_config, config)

        torch.set_grad_enabled(False)
        pipeline = CausalInferencePipeline(args=config, device=device)
        pipeline = pipeline.to(device=device, dtype=torch.bfloat16)
        pipeline.eval()

        num_layers = len(pipeline.generator.model.blocks)
        num_frames = 21
        num_frame_per_block = 3
        frame_seq_length = 1560

        noise = torch.randn(
            [1, num_frames, 16, 60, 104],
            device=device,
            dtype=torch.bfloat16
        )

        ATTENTION_WEIGHT_CAPTURE.enable(
            layer_indices=[0, 4],
            capture_logits=True,
            num_layers=num_layers
        )

        try:
            pipeline.inference(
                noise=noise,
                text_prompts=["A test video"],
                return_latents=True,
            )
            captured = ATTENTION_WEIGHT_CAPTURE.captured_weights.copy()
        finally:
            ATTENTION_WEIGHT_CAPTURE.disable()

        # æŒ‰å±‚åˆ†ç»„ï¼Œå– K æœ€å¤§çš„ï¼ˆæœ€åŽä¸€ä¸ª blockï¼‰
        layer_attentions = {}
        for attn in captured:
            layer_idx = attn['layer_idx']
            if layer_idx not in layer_attentions:
                layer_attentions[layer_idx] = []
            layer_attentions[layer_idx].append(attn)

        for layer_idx in [0, 4]:
            attns = layer_attentions[layer_idx]
            # æŒ‰ K é•¿åº¦æŽ’åº
            attns_sorted = sorted(attns, key=lambda x: x['k_shape'][1], reverse=True)
            best = attns_sorted[0]

            q_len = best['q_shape'][1]
            k_len = best['k_shape'][1]

            # Q åº”è¯¥æ˜¯ 3 å¸§
            expected_q = num_frame_per_block * frame_seq_length
            assert q_len == expected_q, f"Layer {layer_idx}: Q length {q_len} != expected {expected_q}"

            # K åº”è¯¥è‡³å°‘æ˜¯ 18 å¸§ï¼ˆæ‰€æœ‰åŽ†å²å¸§ï¼‰
            min_k = (num_frames - num_frame_per_block) * frame_seq_length
            assert k_len >= min_k, f"Layer {layer_idx}: K length {k_len} < expected minimum {min_k}"

            num_q_frames = q_len // frame_seq_length
            num_k_frames = k_len // frame_seq_length

            print(f"Layer {layer_idx}: Q={num_q_frames}å¸§, K={num_k_frames}å¸§")

        print(f"âœ“ æœ€åŽä¸€ä¸ª block åŒ…å«å®Œæ•´åŽ†å²æµ‹è¯•é€šè¿‡")


if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s", "--tb=short"])
